{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU_7aFidiYnf"
      },
      "source": [
        "## Before Start\n",
        "\n",
        "An initial data exploration was done to first gain some insights. The classes look balanced in the train data at about 50% each, and most reviews are of length 100-200. Even after removing stopwords, the most common word (br) is still a stopword since it is just a break in the text, and does not have any meaning. It should be added to the dictionary to be removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u8Op3ZBJk4KK"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Incredibly intriguing and captivating, I found...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Great movie! oh yeah! Full of energy, full of ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I couldn't believe it when I put this movie in...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Not much to say on this one. A plot you can pr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>With the dialogue in the dubbed version of thi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  labels\n",
              "0  Incredibly intriguing and captivating, I found...       1\n",
              "1  Great movie! oh yeah! Full of energy, full of ...       1\n",
              "2  I couldn't believe it when I put this movie in...       0\n",
              "3  Not much to say on this one. A plot you can pr...       0\n",
              "4  With the dialogue in the dubbed version of thi...       0"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "train_data = pd.read_csv('/Users/kiriharari/Desktop/EE6405/Movie/movie_train.csv')\n",
        "train_data.columns=['text','labels']\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g6INYX0iHdM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# train_data = pd.read_csv('/content/gdrive/MyDrive/news_fulltrain.csv', header=None)\n",
        "# print(train_data.head())\n",
        "\n",
        "print(\"Class Distribution:\")\n",
        "print(train_data['label'].value_counts(normalize=True))\n",
        "\n",
        "review_lengths = train_data['text'].apply(lambda x: len(word_tokenize(x)))\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.hist(review_lengths, bins=50, alpha=0.7)\n",
        "plt.title('Distribution of Review Lengths')\n",
        "plt.xlabel('Reviews Length')\n",
        "plt.ylabel('Reviews Num')\n",
        "plt.show()\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "all_words = [word for review in train_data['text'] for word in word_tokenize(review.lower()) if word.isalpha() and word not in stop_words]\n",
        "\n",
        "word_freq = Counter(all_words).most_common(20)\n",
        "\n",
        "words, frequencies = zip(*word_freq)\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.bar(words, frequencies)\n",
        "plt.title('Top 20 Common Words (Before removing br)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.add('br')\n",
        "all_words = [word for review in train_data['text'] for word in word_tokenize(review.lower()) if word.isalpha() and word not in stop_words]\n",
        "\n",
        "word_freq = Counter(all_words).most_common(20)\n",
        "\n",
        "words, frequencies = zip(*word_freq)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(words, frequencies)\n",
        "plt.title('Top 20 Common Words (After removing br)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "s055gGpWTGYv",
        "outputId": "28a4ddd6-9d23-4a2c-d75b-466e706a51b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7938, 2)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils import resample\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "train_data = pd.read_csv('/Users/kiriharari/Desktop/EE6405/Movie/movie_train.csv')\n",
        "train_data.columns=['text','labels']\n",
        "train_data['labels'].unique()\n",
        "\n",
        "class_0 = train_data[train_data.labels == 0]\n",
        "class_1 = train_data[train_data.labels == 1]\n",
        "majority_class = 0 if (len(class_0)) > (len(class_1)) else 1\n",
        "minority_class = 1 - majority_class\n",
        "\n",
        "#Downsamle\n",
        "class_majority_downsampled = resample(class_0 if majority_class == 0 else class_1,\n",
        "                                      replace=False,\n",
        "                                      n_samples=len(class_1),\n",
        "                                      random_state=42)\n",
        "#Merge balanced data\n",
        "balanced_data = pd.concat([class_majority_downsampled, class_1] if majority_class == 0 else [class_0, class_majority_downsampled])\n",
        "\n",
        "train_balance_data = balanced_data.reset_index(drop=True)\n",
        "\n",
        "test_data = pd.read_csv('/Users/kiriharari/Desktop/EE6405/Movie/movie_test.csv')\n",
        "\n",
        "test_data.columns = ['text', 'labels']\n",
        "class_0 = test_data[test_data.labels == 0]\n",
        "class_1 = test_data[test_data.labels == 1]\n",
        "majority_class = 0 if (len(class_0)) > (len(class_1)) else 1\n",
        "minority_class = 1 - majority_class\n",
        "\n",
        "#Downsamle\n",
        "class_majority_downsampled = resample(class_0 if majority_class == 0 else class_1,\n",
        "                                      replace=False,\n",
        "                                      n_samples=len(class_1),\n",
        "                                      random_state=42)\n",
        "#Merge balanced data\n",
        "balanced_data = pd.concat([class_majority_downsampled, class_1] if majority_class == 0 else [class_0, class_majority_downsampled])\n",
        "\n",
        "test_balance_data = balanced_data.reset_index(drop=True)\n",
        "print(test_balance_data.shape)\n",
        "train_balance_data.to_csv(\"balanced_train.csv\")\n",
        "test_balance_data.to_csv(\"balanced_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfc4ff5722f3451a87fe557e5ac0c8a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/32000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca30c8f015274199b1ed1d7b8c8061eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/7938 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import Dataset, load_metric\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    AutoConfig,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "def tokenize(data):\n",
        "    tokenized_data = tokenizer(data[\"text\"],\n",
        "                     padding = \"max_length\",\n",
        "                     truncation = True,\n",
        "                     max_length=128,\n",
        "                     return_tensors='pt')\n",
        "    tokenized_data['labels'] = torch.tensor(data[\"labels\"])\n",
        "    return tokenized_data\n",
        "# tokenized_train = train_data_balanced.map(tokenize, batched=True)\n",
        "# tokenized_test = test_data_balanced.map(tokenize, batched=True)\n",
        "config = AutoConfig.from_pretrained(\"bert-base-cased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased', num_labels = 2)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_balance_data)\n",
        "test_dataset = Dataset.from_pandas(test_balance_data)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize, batched=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./result\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,                     \n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    f1 = load_metric(\"f1\")\n",
        "    precision = load_metric(\"precision\")\n",
        "    recall = load_metric(\"recall\")\n",
        "    accuracy = load_metric(\"accuracy\")\n",
        "\n",
        "    f1_score = f1.compute(predictions=predictions, references=labels, average='binary')\n",
        "    precision_score = precision.compute(predictions=predictions, references=labels, average='binary')\n",
        "    recall_score = recall.compute(predictions=predictions, references=labels, average='binary')\n",
        "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score['accuracy'],\n",
        "        \"f1\": f1_score['f1'],}\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2243081e436049b297dda21d18f810ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/12000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5031, 'grad_norm': 5.5322089195251465, 'learning_rate': 4.791666666666667e-05, 'epoch': 0.12}\n",
            "{'loss': 0.4444, 'grad_norm': 7.926072597503662, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}\n",
            "{'loss': 0.3981, 'grad_norm': 6.4934868812561035, 'learning_rate': 4.375e-05, 'epoch': 0.38}\n",
            "{'loss': 0.4005, 'grad_norm': 16.78427505493164, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n",
            "{'loss': 0.396, 'grad_norm': 9.174877166748047, 'learning_rate': 3.958333333333333e-05, 'epoch': 0.62}\n",
            "{'loss': 0.3846, 'grad_norm': 8.750117301940918, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}\n",
            "{'loss': 0.3892, 'grad_norm': 10.195127487182617, 'learning_rate': 3.541666666666667e-05, 'epoch': 0.88}\n",
            "{'loss': 0.3826, 'grad_norm': 18.408754348754883, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "434419bbbb374c52ad23b2a44401fa2c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/993 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/6w/klcglgl936g60n8fb40h18500000gn/T/ipykernel_25860/3159577478.py:43: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  f1 = load_metric(\"f1\")\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/precision/precision.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/recall/recall.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3793993890285492, 'eval_accuracy': 0.8652053413958176, 'eval_f1': 0.8701456310679612, 'eval_runtime': 264.9472, 'eval_samples_per_second': 29.961, 'eval_steps_per_second': 3.748, 'epoch': 1.0}\n",
            "{'loss': 0.3178, 'grad_norm': 13.15282154083252, 'learning_rate': 3.125e-05, 'epoch': 1.12}\n",
            "{'loss': 0.2959, 'grad_norm': 0.9394056797027588, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}\n",
            "{'loss': 0.3029, 'grad_norm': 0.46596112847328186, 'learning_rate': 2.7083333333333332e-05, 'epoch': 1.38}\n",
            "{'loss': 0.2975, 'grad_norm': 0.2115456610918045, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n",
            "{'loss': 0.3103, 'grad_norm': 25.81688117980957, 'learning_rate': 2.2916666666666667e-05, 'epoch': 1.62}\n",
            "{'loss': 0.3016, 'grad_norm': 0.06070864200592041, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.75}\n",
            "{'loss': 0.2899, 'grad_norm': 8.122897148132324, 'learning_rate': 1.8750000000000002e-05, 'epoch': 1.88}\n",
            "{'loss': 0.2805, 'grad_norm': 8.181387901306152, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "691a11a204f741929e9321780e167e7a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/993 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/precision/precision.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/recall/recall.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4966689348220825, 'eval_accuracy': 0.8677248677248677, 'eval_f1': 0.8635654885654886, 'eval_runtime': 230.6883, 'eval_samples_per_second': 34.41, 'eval_steps_per_second': 4.305, 'epoch': 2.0}\n",
            "{'loss': 0.1749, 'grad_norm': 45.78474807739258, 'learning_rate': 1.4583333333333335e-05, 'epoch': 2.12}\n",
            "{'loss': 0.1606, 'grad_norm': 0.10074486583471298, 'learning_rate': 1.25e-05, 'epoch': 2.25}\n",
            "{'loss': 0.1845, 'grad_norm': 88.7400894165039, 'learning_rate': 1.0416666666666668e-05, 'epoch': 2.38}\n",
            "{'loss': 0.1663, 'grad_norm': 0.10826538503170013, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n",
            "{'loss': 0.174, 'grad_norm': 0.194576695561409, 'learning_rate': 6.25e-06, 'epoch': 2.62}\n",
            "{'loss': 0.1643, 'grad_norm': 9.042320251464844, 'learning_rate': 4.166666666666667e-06, 'epoch': 2.75}\n",
            "{'loss': 0.1665, 'grad_norm': 0.11958027631044388, 'learning_rate': 2.0833333333333334e-06, 'epoch': 2.88}\n",
            "{'loss': 0.1431, 'grad_norm': 0.15002697706222534, 'learning_rate': 0.0, 'epoch': 3.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b0d052f2ad54b7a88c915a015801757",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/993 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/precision/precision.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/recall/recall.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.5846240520477295, 'eval_accuracy': 0.8751574703955657, 'eval_f1': 0.8760165144501438, 'eval_runtime': 203.2291, 'eval_samples_per_second': 39.059, 'eval_steps_per_second': 4.886, 'epoch': 3.0}\n",
            "{'train_runtime': 11145.5319, 'train_samples_per_second': 8.613, 'train_steps_per_second': 1.077, 'train_loss': 0.2928744951883952, 'epoch': 3.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=12000, training_loss=0.2928744951883952, metrics={'train_runtime': 11145.5319, 'train_samples_per_second': 8.613, 'train_steps_per_second': 1.077, 'train_loss': 0.2928744951883952, 'epoch': 3.0})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17d7ad5cb614457e90335f5136b96a26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/993 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/precision/precision.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/recall/recall.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.5846240520477295,\n",
              " 'eval_accuracy': 0.8751574703955657,\n",
              " 'eval_f1': 0.8760165144501438,\n",
              " 'eval_runtime': 172.513,\n",
              " 'eval_samples_per_second': 46.014,\n",
              " 'eval_steps_per_second': 5.756,\n",
              " 'epoch': 3.0}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/Users/kiriharari/Desktop/EE6405/Movie/saved_model/tokenizer_config.json',\n",
              " '/Users/kiriharari/Desktop/EE6405/Movie/saved_model/special_tokens_map.json',\n",
              " '/Users/kiriharari/Desktop/EE6405/Movie/saved_model/vocab.txt',\n",
              " '/Users/kiriharari/Desktop/EE6405/Movie/saved_model/added_tokens.json',\n",
              " '/Users/kiriharari/Desktop/EE6405/Movie/saved_model/tokenizer.json')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# æŒ‡å®šä¿å­˜æ¨¡åž‹çš„è·¯å¾„\n",
        "model_path = \"/Users/kiriharari/Desktop/EE6405/Movie/saved_model\"\n",
        "\n",
        "# ä¿å­˜æ¨¡åž‹\n",
        "model.save_pretrained(model_path)\n",
        "\n",
        "# åŒæ—¶ï¼Œä¿å­˜ tokenizer åˆ°ç›¸åŒçš„è·¯å¾„\n",
        "tokenizer.save_pretrained(model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9OCWNfaPpuzf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d7d60a584c34bb9bef9d9cf768ed04e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/993 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/precision/precision.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/recall/recall.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.5846240520477295,\n",
              " 'eval_accuracy': 0.8751574703955657,\n",
              " 'eval_f1': 0.8760165144501438,\n",
              " 'eval_precision': 0.8700298210735586,\n",
              " 'eval_recall': 0.8820861678004536,\n",
              " 'eval_runtime': 170.588,\n",
              " 'eval_samples_per_second': 46.533,\n",
              " 'eval_steps_per_second': 5.821}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import Dataset, load_metric\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    AutoConfig,\n",
        ")\n",
        "model_path = \"/Users/kiriharari/Desktop/EE6405/Movie/saved_model\"\n",
        "\n",
        "# åŠ è½½æ¨¡åž‹\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# åŠ è½½ tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    f1 = load_metric(\"f1\")\n",
        "    precision = load_metric(\"precision\")\n",
        "    recall = load_metric(\"recall\")\n",
        "    accuracy = load_metric(\"accuracy\")\n",
        "\n",
        "    f1_score = f1.compute(predictions=predictions, references=labels, average='binary')\n",
        "    precision_score = precision.compute(predictions=predictions, references=labels, average='binary')\n",
        "    recall_score = recall.compute(predictions=predictions, references=labels, average='binary')\n",
        "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score['accuracy'],\n",
        "        \"f1\": f1_score['f1'],\n",
        "        \"precision\": precision_score['precision'],\n",
        "        \"recall\": recall_score['recall']\n",
        "    }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Satire\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb9f34b327cb4562970a01acf31670cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "968cc584d8cf4dac93c2bf23a130a04b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e32134a0e7814361bd992c4741944e25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilities: tensor([[0.9943, 0.0057]])\n",
            "Predictions: tensor([0])\n",
            "Reliable\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c15c16bb19647e0b97bcee9140dfbb8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilities: tensor([[0.2309, 0.7691]])\n",
            "Predictions: tensor([1])\n"
          ]
        }
      ],
      "source": [
        "test_string_satire = '''\n",
        "When so many actors seem content to churn out performances for a quick paycheck, a performer who adheres to his principles really stands out. Thats why Jeff Bridges made waves this week when he announced that from now on, he will only perform nude scenes. In an interview in this months GQ, the Big Lebowski star made it clear that he was more than ready to move on to a new phase in his career, leaving his clothed roles in the past. Ive been there and Ive done that, said Bridges, rattling off a laundry list of the films hes appeared in covered up. Now, I can finally afford to only take on roles that excite me. Right now, those are roles with nude scenes. Why waste my time with anything else? Powerful. Though he made it clear that he doesnt regret his previous non-nude roles, Jeff admitted that hed always struggled with pressure from directors and studios to stay clothed on camera. No more towels; no more bathrobes; no more carefully placed plants, he added. Even if my character isnt written as nude, any director I work with will have to figure out how to make him that way. Itll be a challenge for both of us, and one I cant wait to tackle. For their part, Jeffs fans have been nothing but supportive. Wow! Whether or not you agree with him, youve got to have respect for a Hollywood star with that much conviction. You keep doing you, Jeff!\n",
        "'''\n",
        "\n",
        "test_string_reliable = '''\n",
        "The Alberta province health minister wants to know if swine flu shots were 'inappropriately diverted' to the Calgary Flames while thousands had to stand in line for hours for the vaccine. Alberta Health Minister Ron Liepert says he doesn't know where the NHL team got the vaccine, adding that Alberta Health Services is the only supplier in the province. Team president Ken King says the club contacted the department and asked for the clinic. Health officials have begun an investigation into the special clinic, which was held for the players and their families last Friday. Liepert says the vaccine would be diverted only with the approval of the chief medical officer of health, but he doesn't know if that was the case. Alberta's opposition parties say professional ice hockey players shouldn't be getting the vaccine ahead of cancer patients and pregnant women.\n",
        "'''\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_eval_batch_size=1  # è®¾ç½®ä¸º1å› ä¸ºæˆ‘ä»¬åªé¢„æµ‹ä¸€ä¸ªæ ·æœ¬\n",
        ")\n",
        "trainer = Trainer(model=model, args=training_args)\n",
        "print(\"Satire\")\n",
        "satire_input_x = pd.DataFrame({'labels':[0], 'text': test_string_satire})\n",
        "\n",
        "reliable_input_x = pd.DataFrame({'labels':[0], 'text': test_string_reliable})\n",
        "\n",
        "satire_dataset = Dataset.from_pandas(satire_input_x)\n",
        "reliable_dataset = Dataset.from_pandas(reliable_input_x)\n",
        "\n",
        "satire_input_x = satire_dataset.map(tokenize, batched=True)\n",
        "reliable_input_x = reliable_dataset.map(tokenize, batched=True)\n",
        "\n",
        "# satire_input_x['labels'] = torch.tensor(0)\n",
        "# Check the number of tokens\n",
        "# print(\"Input IDs shape:\", satire_input_x['input_ids'].shape)  # åº”æ˜¾ç¤º (1, 128) æˆ–å…¶ä»–ç±»ä¼¼æ‰¹å¤„ç†å½¢çŠ¶\n",
        "# print(\"Input IDs shape:\", tokenized_test['input_ids'].shape) \n",
        "predictions = trainer.predict(satire_input_x)\n",
        "\n",
        "\n",
        "logits = predictions.predictions\n",
        "probs = torch.softmax(torch.tensor(logits), dim=-1)\n",
        "predictions = torch.argmax(probs, dim=-1)\n",
        "\n",
        "print(\"Probabilities:\", probs)\n",
        "print(\"Predictions:\", predictions)\n",
        "\n",
        "print(\"Reliable\")\n",
        "\n",
        "predictions = trainer.predict(reliable_input_x)\n",
        "\n",
        "\n",
        "logits = predictions.predictions\n",
        "probs = torch.softmax(torch.tensor(logits), dim=-1)\n",
        "predictions = torch.argmax(probs, dim=-1)\n",
        "\n",
        "print(\"Probabilities:\", probs)\n",
        "print(\"Predictions:\", predictions)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

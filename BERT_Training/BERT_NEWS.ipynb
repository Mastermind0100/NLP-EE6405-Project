{"cells":[{"cell_type":"markdown","metadata":{"id":"TU_7aFidiYnf"},"source":["## Before Start\n","\n","An initial data exploration was done to first gain some insights. The classes look balanced in the train data at about 50% each, and most reviews are of length 100-200. Even after removing stopwords, the most common word (br) is still a stopword since it is just a break in the text, and does not have any meaning. It should be added to the dictionary to be removed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u8Op3ZBJk4KK"},"outputs":[],"source":["import pandas as pd\n","train_data = pd.read_csv('/Users/kiriharari/Downloads/news_fulltrain.csv', header=None)\n","train_data.columns=['labels','text']\n","train_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8g6INYX0iHdM"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","from nltk.tokenize import word_tokenize\n","\n","# train_data = pd.read_csv('/content/gdrive/MyDrive/news_fulltrain.csv', header=None)\n","# print(train_data.head())\n","\n","print(\"Class Distribution:\")\n","print(train_data['label'].value_counts(normalize=True))\n","\n","review_lengths = train_data['text'].apply(lambda x: len(word_tokenize(x)))\n","plt.figure(figsize=(9, 6))\n","plt.hist(review_lengths, bins=50, alpha=0.7)\n","plt.title('Distribution of Review Lengths')\n","plt.xlabel('Reviews Length')\n","plt.ylabel('Reviews Num')\n","plt.show()\n","\n","stop_words = set(stopwords.words('english'))\n","all_words = [word for review in train_data['text'] for word in word_tokenize(review.lower()) if word.isalpha() and word not in stop_words]\n","\n","word_freq = Counter(all_words).most_common(20)\n","\n","words, frequencies = zip(*word_freq)\n","plt.figure(figsize=(9, 6))\n","plt.bar(words, frequencies)\n","plt.title('Top 20 Common Words (Before removing br)')\n","plt.xticks(rotation=45)\n","plt.xlabel('Words')\n","plt.ylabel('Frequency')\n","plt.show()\n","\n","stop_words = set(stopwords.words('english'))\n","stop_words.add('br')\n","all_words = [word for review in train_data['text'] for word in word_tokenize(review.lower()) if word.isalpha() and word not in stop_words]\n","\n","word_freq = Counter(all_words).most_common(20)\n","\n","words, frequencies = zip(*word_freq)\n","plt.figure(figsize=(10, 6))\n","plt.bar(words, frequencies)\n","plt.title('Top 20 Common Words (After removing br)')\n","plt.xticks(rotation=45)\n","plt.xlabel('Words')\n","plt.ylabel('Frequency')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s055gGpWTGYv"},"outputs":[],"source":["from sklearn.utils import resample\n","import torch\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","import numpy as np\n","# import os\n","# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","train_data = pd.read_csv('/Users/kiriharari/Desktop/EE6405/BERT_NEWS/news_fulltrain.csv', header=None)\n","train_data.columns=['labels','text']\n","train_data = train_data[(train_data['labels'] == 1) | (train_data['labels'] == 4)]\n","train_data['labels'].unique()\n","\n","train_data_satire  =train_data[train_data['labels'] == 1]\n","train_data_reliable  =train_data[train_data['labels'] == 4]\n","\n","train_data_satire_sampled = resample(train_data_satire,\n","                                     replace=True,\n","                                     n_samples=len(train_data_satire),\n","                                     random_state=42)\n","\n","train_data_balanced = pd.concat([train_data_satire_sampled, train_data_reliable])\n","train_data_balanced['labels'] = train_data_balanced['labels'].replace({1:0, 4:1})\n","\n","\n","test_data = pd.read_csv('/Users/kiriharari/Desktop/EE6405/BERT_NEWS/news_balancedtest.csv')\n","\n","test_data.columns = ['labels', 'text']\n","test_data = test_data[(test_data['labels'] == 1) | (test_data['labels'] == 4)]\n","\n","test_data_balanced = test_data.copy()\n","test_data_balanced['labels'] = test_data_balanced['labels'].replace({1: 0, 4: 1})\n","test_data_balanced.head()\n","train_data_balanced.head()\n","train_data_balanced = train_data_balanced.reset_index(drop=True)\n","test_data_balanced = test_data_balanced.reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"utrppHsr4aGb"},"outputs":[],"source":["import torch\n","from datasets import Dataset, load_metric\n","import evaluate\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    TrainingArguments,\n","    Trainer,\n","    AutoConfig,\n",")\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","def tokenize(data):\n","    tokenized_data = tokenizer(data[\"text\"],\n","                     padding = \"max_length\",\n","                     truncation = True,\n","                     max_length=128,\n","                     return_tensors='pt')\n","    tokenized_data['labels'] = torch.tensor(data[\"labels\"])\n","    return tokenized_data\n","# tokenized_train = train_data_balanced.map(tokenize, batched=True)\n","# tokenized_test = test_data_balanced.map(tokenize, batched=True)\n","config = AutoConfig.from_pretrained(\"bert-base-cased\")\n","model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased', num_labels = 2)\n","\n","train_dataset = Dataset.from_pandas(train_data_balanced)\n","test_dataset = Dataset.from_pandas(test_data_balanced)\n","\n","tokenized_train = train_dataset.map(tokenize, batched=True)\n","tokenized_test = test_dataset.map(tokenize, batched=True)\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./result\",\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=5e-5,\n",")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=1)\n","    f1 = load_metric(\"f1\")\n","    precision = load_metric(\"precision\")\n","    recall = load_metric(\"recall\")\n","    accuracy = load_metric(\"accuracy\")\n","\n","    f1_score = f1.compute(predictions=predictions, references=labels, average='binary')\n","    precision_score = precision.compute(predictions=predictions, references=labels, average='binary')\n","    recall_score = recall.compute(predictions=predictions, references=labels, average='binary')\n","    accuracy_score = accuracy.compute(predictions=predictions, references=labels)\n","\n","    return {\n","        \"accuracy\": accuracy_score['accuracy'],\n","        \"f1\": f1_score['f1'],}\n","# Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train,\n","    eval_dataset=tokenized_test,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hn79tCgx4aGc"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEvChS4_4aGc"},"outputs":[],"source":["trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iMFA2oBl4aGc"},"outputs":[],"source":["# 指定保存模型的路径\n","model_path = \"/Users/kiriharari/Desktop/EE6405/BERT_NEWS/saved_model\"\n","\n","# 保存模型\n","model.save_pretrained(model_path)\n","\n","# 同时，保存 tokenizer 到相同的路径\n","tokenizer.save_pretrained(model_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9OCWNfaPpuzf"},"outputs":[],"source":["import torch\n","from datasets import Dataset, load_metric\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    TrainingArguments,\n","    Trainer,\n","    AutoConfig,\n",")\n","model_path = \"/Users/kiriharari/Desktop/EE6405/BERT_NEWS/saved_model\"\n","\n","# 加载模型\n","model = AutoModelForSequenceClassification.from_pretrained(model_path)\n","\n","# 加载 tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=1)\n","    f1 = load_metric(\"f1\")\n","    precision = load_metric(\"precision\")\n","    recall = load_metric(\"recall\")\n","    accuracy = load_metric(\"accuracy\")\n","\n","    f1_score = f1.compute(predictions=predictions, references=labels, average='binary')\n","    precision_score = precision.compute(predictions=predictions, references=labels, average='binary')\n","    recall_score = recall.compute(predictions=predictions, references=labels, average='binary')\n","    accuracy_score = accuracy.compute(predictions=predictions, references=labels)\n","\n","    return {\n","        \"accuracy\": accuracy_score['accuracy'],\n","        \"f1\": f1_score['f1'],\n","        \"precision\": precision_score['precision'],\n","        \"recall\": recall_score['recall']\n","    }\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train,\n","    eval_dataset=tokenized_test,\n","    compute_metrics=compute_metrics\n",")\n","\n","trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AbU9Shec4aGc"},"outputs":[],"source":["test_string_satire = '''\n","When so many actors seem content to churn out performances for a quick paycheck, a performer who adheres to his principles really stands out. Thats why Jeff Bridges made waves this week when he announced that from now on, he will only perform nude scenes. In an interview in this months GQ, the Big Lebowski star made it clear that he was more than ready to move on to a new phase in his career, leaving his clothed roles in the past. Ive been there and Ive done that, said Bridges, rattling off a laundry list of the films hes appeared in covered up. Now, I can finally afford to only take on roles that excite me. Right now, those are roles with nude scenes. Why waste my time with anything else? Powerful. Though he made it clear that he doesnt regret his previous non-nude roles, Jeff admitted that hed always struggled with pressure from directors and studios to stay clothed on camera. No more towels; no more bathrobes; no more carefully placed plants, he added. Even if my character isnt written as nude, any director I work with will have to figure out how to make him that way. Itll be a challenge for both of us, and one I cant wait to tackle. For their part, Jeffs fans have been nothing but supportive. Wow! Whether or not you agree with him, youve got to have respect for a Hollywood star with that much conviction. You keep doing you, Jeff!\n","'''\n","\n","test_string_reliable = '''\n","The Alberta province health minister wants to know if swine flu shots were 'inappropriately diverted' to the Calgary Flames while thousands had to stand in line for hours for the vaccine. Alberta Health Minister Ron Liepert says he doesn't know where the NHL team got the vaccine, adding that Alberta Health Services is the only supplier in the province. Team president Ken King says the club contacted the department and asked for the clinic. Health officials have begun an investigation into the special clinic, which was held for the players and their families last Friday. Liepert says the vaccine would be diverted only with the approval of the chief medical officer of health, but he doesn't know if that was the case. Alberta's opposition parties say professional ice hockey players shouldn't be getting the vaccine ahead of cancer patients and pregnant women.\n","'''\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    per_device_eval_batch_size=1  # 设置为1因为我们只预测一个样本\n",")\n","trainer = Trainer(model=model, args=training_args)\n","print(\"Satire\")\n","satire_input_x = pd.DataFrame({'labels':[0], 'text': test_string_satire})\n","\n","reliable_input_x = pd.DataFrame({'labels':[0], 'text': test_string_reliable})\n","\n","satire_dataset = Dataset.from_pandas(satire_input_x)\n","reliable_dataset = Dataset.from_pandas(reliable_input_x)\n","\n","satire_input_x = satire_dataset.map(tokenize, batched=True)\n","reliable_input_x = reliable_dataset.map(tokenize, batched=True)\n","\n","# satire_input_x['labels'] = torch.tensor(0)\n","# Check the number of tokens\n","# print(\"Input IDs shape:\", satire_input_x['input_ids'].shape)  # 应显示 (1, 128) 或其他类似批处理形状\n","# print(\"Input IDs shape:\", tokenized_test['input_ids'].shape)\n","predictions = trainer.predict(satire_input_x)\n","\n","\n","logits = predictions.predictions\n","probs = torch.softmax(torch.tensor(logits), dim=-1)\n","predictions = torch.argmax(probs, dim=-1)\n","\n","print(\"Probabilities:\", probs)\n","print(\"Predictions:\", predictions)\n","\n","print(\"Reliable\")\n","\n","predictions = trainer.predict(reliable_input_x)\n","\n","\n","logits = predictions.predictions\n","probs = torch.softmax(torch.tensor(logits), dim=-1)\n","predictions = torch.argmax(probs, dim=-1)\n","\n","print(\"Probabilities:\", probs)\n","print(\"Predictions:\", predictions)"]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[]},"kernelspec":{"display_name":"dl","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
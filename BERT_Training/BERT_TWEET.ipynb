{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU_7aFidiYnf"
      },
      "source": [
        "## Before Start\n",
        "\n",
        "An initial data exploration was done to first gain some insights. The classes look balanced in the train data at about 50% each, and most reviews are of length 100-200. Even after removing stopwords, the most common word (br) is still a stopword since it is just a break in the text, and does not have any meaning. It should be added to the dictionary to be removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "u8Op3ZBJk4KK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  labels                                               text\n",
            "1      0   @user when a father is dysfunctional and is s...\n",
            "2      0  @user @user thanks for #lyft credit i can't us...\n",
            "3      0                                bihday your majesty\n",
            "4      0  #model   i love u take with u all the time in ...\n",
            "5      0             factsguide: society now    #motivation\n",
            "Class Distribution:\n",
            "labels\n",
            "0    0.929854\n",
            "1    0.070146\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "data = pd.read_csv('/Users/kiriharari/Desktop/EE6405/BERT_TWEET/twitter_E6oV3lV.csv', header=None)\n",
        "data = data.iloc[1:, 1:]\n",
        "data.columns=['labels','text']\n",
        "print(data.head())\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.add('br')\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_text)\n",
        "\n",
        "def preprocess_text(data):\n",
        "    data['text'] = data['text'].apply(remove_stopwords)#Remove stopwords\n",
        "    data['text'] = data['text'].apply(lambda x: re.sub('https?:\\/\\/.*[\\r\\n]*', ' ', x))#Remove URLs\n",
        "    data['text'] = data['text'].apply(lambda x: re.sub('[^a-zA-Z0-9 \\n]', ' ', x))#Remove non-alphanumeric characters\n",
        "    data['text'] = data['text'].apply(lambda x: re.sub('@[\\w]*', '', x))#Remove Twitter usernames\n",
        "    data['text'] = data['text'].apply(lambda x: re.sub('\\d+', ' ', x))#Remove digits\n",
        "    data['text'] = data['text'].apply(lambda x: re.sub('user', '', x))#Remove Twitter usernames\n",
        "    data['text'] = data['text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>1]))#Remove Short Words\n",
        "    data['text'] = data['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))#Remove punctuation\n",
        "    data['text'] = data['text'].str.lower()\n",
        "    return data\n",
        "print(\"Class Distribution:\")\n",
        "print(data['labels'].value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class Distribution:\n",
            "labels\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n",
            "      labels                                               text\n",
            "8825       0  #body to body massage with a   ending oil #mas...\n",
            "31855      0   @user @ my call back!  #casting #castingcall ...\n",
            "28080      0  help creates the #environment of #togetherness...\n",
            "29215      0  summer with friendâ¨ð¥ #summer  #friend #li...\n",
            "20026      0  follow me on snapchat at awesomecutenes7 #snap...\n",
            "...      ...                                                ...\n",
            "31935      1  lady banned from kentucky mall. @user  #jcpenn...\n",
            "31947      1  @user omfg i'm offended! i'm a  mailbox and i'...\n",
            "31948      1  @user @user you don't have the balls to hashta...\n",
            "31949      1   makes you ask yourself, who am i? then am i a...\n",
            "31961      1  @user #sikh #temple vandalised in in #calgary,...\n",
            "\n",
            "[4484 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "class_0 = data[data.labels == '0']\n",
        "class_1 = data[data.labels == '1']\n",
        "majority_class = 0 if (len(class_0)) > (len(class_1)) else 1\n",
        "minority_class = 1 - majority_class\n",
        "\n",
        "#Downsamle\n",
        "class_majority_downsampled = resample(class_0 if majority_class == 0 else class_1,\n",
        "                                      replace=False,\n",
        "                                      n_samples=len(class_1),\n",
        "                                      random_state=42)\n",
        "#Merge balanced data\n",
        "balanced_data = pd.concat([class_majority_downsampled, class_1] if majority_class == 0 else [class_0, class_majority_downsampled])\n",
        "#print(balanced_data['label'].value_counts())\n",
        "print(\"Class Distribution:\")\n",
        "print(balanced_data['labels'].value_counts(normalize=True))\n",
        "print(balanced_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [labels, text]\n",
              "Index: []"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "train_data, test_data = train_test_split(balanced_data, test_size=0.2, random_state=42)\n",
        "train_data = preprocess_text(train_data)\n",
        "test_data = preprocess_text(test_data)\n",
        "train_data.to_csv('train_data.csv', index=False)\n",
        "test_data.to_csv('test_data.csv', index=False)\n",
        "\n",
        "train_data = pd.read_csv('train_data.csv')\n",
        "test_data = pd.read_csv('test_data.csv')\n",
        "\n",
        "train_data.columns=['labels','text']\n",
        "test_data.columns=['labels','text']\n",
        "\n",
        "train_data.head()\n",
        "train_data = train_data.dropna()\n",
        "test_data = test_data.dropna()\n",
        "train_data[train_data.isnull().values == True]\n",
        "test_data[test_data.isnull().values == True]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "s055gGpWTGYv",
        "outputId": "28a4ddd6-9d23-4a2c-d75b-466e706a51b1"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import resample\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# train_data = pd.read_csv('/Users/kiriharari/Downloads/news_fulltrain.csv', header=None)\n",
        "train_data.columns=['labels','text']\n",
        "# train_data = train_data[(train_data['labels'] == 1) | (train_data['labels'] == 4)]\n",
        "train_data['labels'].unique()\n",
        "train_data_balanced = train_data.copy()\n",
        "\n",
        "\n",
        "test_data.columns = ['labels', 'text']\n",
        "# test_data = test_data[(test_data['labels'] == 1) | (test_data['labels'] == 4)]\n",
        "\n",
        "test_data_balanced = test_data.copy()\n",
        "test_data_balanced.head()\n",
        "train_data_balanced.head()\n",
        "# test_data.columns = ['label', 0, 'text', 1,2,3,4,5,6]\n",
        "# test_data = test_data.loc[:, ['label', 'text']]\n",
        "\n",
        "# train_data['text'] = train_data['text'].apply(remove_stopwords)\n",
        "# test_data['text'] = test_data['text'].apply(remove_stopwords)\n",
        "\n",
        "# #limit set to not run out of ram\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# def tokenize_and_encode(data):\n",
        "#     inputs = tokenizer.batch_encode_plus(\n",
        "#         data['text'].tolist(),\n",
        "#         add_special_tokens=True,\n",
        "#         max_length=128,\n",
        "#         padding='max_length',\n",
        "#         truncation=True,\n",
        "#         return_tensors='pt'\n",
        "#     )\n",
        "#     labels = torch.tensor(data['label'].tolist())\n",
        "#     return TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
        "\n",
        "# train_dataset = tokenize_and_encode(train_data)\n",
        "# test_dataset = tokenize_and_encode(test_data)\n",
        "\n",
        "# train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)\n",
        "# test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=16)\n",
        "train_data_balanced = train_data_balanced.reset_index(drop=True)\n",
        "test_data_balanced = test_data_balanced.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "860ffc6f51a74c62b2b8f21415aaefcc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3582 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23659a46b89148edbde0209b8f1d10bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/895 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import Dataset, load_metric\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    AutoConfig,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "def tokenize(data):\n",
        "    tokenized_data = tokenizer(data[\"text\"],\n",
        "                     padding = \"max_length\",\n",
        "                     truncation = True,\n",
        "                     max_length=128,\n",
        "                     return_tensors='pt')\n",
        "    tokenized_data['labels'] = torch.tensor(data[\"labels\"])\n",
        "    return tokenized_data\n",
        "# tokenized_train = train_data_balanced.map(tokenize, batched=True)\n",
        "# tokenized_test = test_data_balanced.map(tokenize, batched=True)\n",
        "config = AutoConfig.from_pretrained(\"bert-base-cased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased', num_labels = 2)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_data_balanced)\n",
        "test_dataset = Dataset.from_pandas(test_data_balanced)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize, batched=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./result\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,                     \n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    f1 = load_metric(\"f1\")\n",
        "    precision = load_metric(\"precision\")\n",
        "    recall = load_metric(\"recall\")\n",
        "    accuracy = load_metric(\"accuracy\")\n",
        "\n",
        "    f1_score = f1.compute(predictions=predictions, references=labels, average='binary')\n",
        "    precision_score = precision.compute(predictions=predictions, references=labels, average='binary')\n",
        "    recall_score = recall.compute(predictions=predictions, references=labels, average='binary')\n",
        "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score['accuracy'],\n",
        "        \"f1\": f1_score['f1'],\n",
        "        \"precision\": precision_score['precision'],\n",
        "        \"recall\": recall_score['recall']\n",
        "    }\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "print(len(set(tokenized_train['labels'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cdace1a749e4b7d8307fdc41cc93379",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1344 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "895294b226b24ac6812cb597064546f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/112 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/precision/precision.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/recall/recall.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.368895560503006, 'eval_accuracy': 0.8592178770949721, 'eval_f1': 0.859375, 'eval_precision': 0.8690744920993227, 'eval_recall': 0.8498896247240618, 'eval_runtime': 24.2523, 'eval_samples_per_second': 36.904, 'eval_steps_per_second': 4.618, 'epoch': 1.0}\n",
            "{'loss': 0.3997, 'grad_norm': 0.10481197386980057, 'learning_rate': 3.1398809523809525e-05, 'epoch': 1.12}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbba4a3216d34a5bba4f8a3fa0ae5b22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/112 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/precision/precision.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/recall/recall.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.42266491055488586, 'eval_accuracy': 0.8726256983240224, 'eval_f1': 0.878723404255319, 'eval_precision': 0.8480492813141683, 'eval_recall': 0.9116997792494481, 'eval_runtime': 25.1202, 'eval_samples_per_second': 35.629, 'eval_steps_per_second': 4.459, 'epoch': 2.0}\n",
            "{'loss': 0.2308, 'grad_norm': 0.4954136610031128, 'learning_rate': 1.2797619047619047e-05, 'epoch': 2.23}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90ee3b3ea2e74a68a20235d8760a176c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/112 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/precision/precision.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/recall/recall.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.5378416180610657, 'eval_accuracy': 0.88268156424581, 'eval_f1': 0.8834628190899, 'eval_precision': 0.8883928571428571, 'eval_recall': 0.8785871964679912, 'eval_runtime': 30.0671, 'eval_samples_per_second': 29.767, 'eval_steps_per_second': 3.725, 'epoch': 3.0}\n",
            "{'train_runtime': 1113.3128, 'train_samples_per_second': 9.652, 'train_steps_per_second': 1.207, 'train_loss': 0.26385126795087543, 'epoch': 3.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1344, training_loss=0.26385126795087543, metrics={'train_runtime': 1113.3128, 'train_samples_per_second': 9.652, 'train_steps_per_second': 1.207, 'train_loss': 0.26385126795087543, 'epoch': 3.0})"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77f68be3b3e54d398d549f4f2bf7dbab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/112 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/precision/precision.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/recall/recall.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.5378416180610657,\n",
              " 'eval_accuracy': 0.88268156424581,\n",
              " 'eval_f1': 0.8834628190899,\n",
              " 'eval_precision': 0.8883928571428571,\n",
              " 'eval_recall': 0.8785871964679912,\n",
              " 'eval_runtime': 27.9163,\n",
              " 'eval_samples_per_second': 32.06,\n",
              " 'eval_steps_per_second': 4.012,\n",
              " 'epoch': 3.0}"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/Users/kiriharari/Desktop/EE6405/BERT_TWEET/saved_model/tokenizer_config.json',\n",
              " '/Users/kiriharari/Desktop/EE6405/BERT_TWEET/saved_model/special_tokens_map.json',\n",
              " '/Users/kiriharari/Desktop/EE6405/BERT_TWEET/saved_model/vocab.txt',\n",
              " '/Users/kiriharari/Desktop/EE6405/BERT_TWEET/saved_model/added_tokens.json',\n",
              " '/Users/kiriharari/Desktop/EE6405/BERT_TWEET/saved_model/tokenizer.json')"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 指定保存模型的路径\n",
        "model_path = \"/Users/kiriharari/Desktop/EE6405/BERT_TWEET/saved_model\"\n",
        "\n",
        "# 保存模型\n",
        "model.save_pretrained(model_path)\n",
        "\n",
        "# 同时，保存 tokenizer 到相同的路径\n",
        "tokenizer.save_pretrained(model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8a4ccc366ae487f90f92378146c7d7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/112 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/precision/precision.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/recall/recall.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/Users/kiriharari/miniconda3/envs/dl/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.5378416180610657,\n",
              " 'eval_accuracy': 0.88268156424581,\n",
              " 'eval_f1': 0.8834628190899,\n",
              " 'eval_precision': 0.8883928571428571,\n",
              " 'eval_recall': 0.8785871964679912,\n",
              " 'eval_runtime': 24.0661,\n",
              " 'eval_samples_per_second': 37.189,\n",
              " 'eval_steps_per_second': 4.654}"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "\n",
        "model_path = \"/Users/kiriharari/Desktop/EE6405/BERT_TWEET/saved_model\"\n",
        "\n",
        "# 加载模型\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# 加载 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    f1 = load_metric(\"f1\")\n",
        "    precision = load_metric(\"precision\")\n",
        "    recall = load_metric(\"recall\")\n",
        "    accuracy = load_metric(\"accuracy\")\n",
        "\n",
        "    f1_score = f1.compute(predictions=predictions, references=labels, average='binary')\n",
        "    precision_score = precision.compute(predictions=predictions, references=labels, average='binary')\n",
        "    recall_score = recall.compute(predictions=predictions, references=labels, average='binary')\n",
        "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score['accuracy'],\n",
        "        \"f1\": f1_score['f1'],\n",
        "        \"precision\": precision_score['precision'],\n",
        "        \"recall\": recall_score['recall']\n",
        "    }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPIqal9pprLl"
      },
      "source": [
        "## This is a SAMPLE TEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OCWNfaPpuzf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
